{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nprint(\"GPU:\", tf.config.list_physical_devices('GPU'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T20:27:23.021007Z","iopub.execute_input":"2025-12-19T20:27:23.021224Z","iopub.status.idle":"2025-12-19T20:27:38.731448Z","shell.execute_reply.started":"2025-12-19T20:27:23.021198Z","shell.execute_reply":"2025-12-19T20:27:38.730801Z"}},"outputs":[{"name":"stderr","text":"2025-12-19 20:27:24.790923: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766176044.996734      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766176045.050000      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766176045.537789      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766176045.537845      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766176045.537850      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766176045.537853      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import kagglehub\nimport os\n\npath = kagglehub.dataset_download(\"xdxd003/ff-c23\")\n\nprint(\"Dataset downloaded to:\")\nprint(path)\n\nprint(\"\\nContents:\")\nprint(os.listdir(path))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T20:28:32.607518Z","iopub.execute_input":"2025-12-19T20:28:32.608313Z","iopub.status.idle":"2025-12-19T20:28:32.709748Z","shell.execute_reply.started":"2025-12-19T20:28:32.608277Z","shell.execute_reply":"2025-12-19T20:28:32.709046Z"}},"outputs":[{"name":"stdout","text":"Dataset downloaded to:\n/kaggle/input/ff-c23\n\nContents:\n['FaceForensics++_C23']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\n\nDATASET_PATH = \"/kaggle/input/ff-c23/FaceForensics++_C23\"\nprint(os.listdir(DATASET_PATH))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T20:28:50.147228Z","iopub.execute_input":"2025-12-19T20:28:50.147529Z","iopub.status.idle":"2025-12-19T20:28:50.154723Z","shell.execute_reply.started":"2025-12-19T20:28:50.147502Z","shell.execute_reply":"2025-12-19T20:28:50.154032Z"}},"outputs":[{"name":"stdout","text":"['Face2Face', 'csv', 'Deepfakes', 'DeepFakeDetection', 'original', 'NeuralTextures', 'FaceShifter', 'FaceSwap']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\n\nDATASET_PATH = \"/kaggle/input/ff-c23/FaceForensics++_C23\"\n\nREAL_DIR = os.path.join(DATASET_PATH, \"original\")\nFAKE_DIR = os.path.join(DATASET_PATH, \"Deepfakes\")\n\nprint(\"Real videos:\", len(os.listdir(REAL_DIR)))\nprint(\"Fake videos:\", len(os.listdir(FAKE_DIR)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T20:30:06.647195Z","iopub.execute_input":"2025-12-19T20:30:06.647731Z","iopub.status.idle":"2025-12-19T20:30:06.653921Z","shell.execute_reply.started":"2025-12-19T20:30:06.647701Z","shell.execute_reply":"2025-12-19T20:30:06.653166Z"}},"outputs":[{"name":"stdout","text":"Real videos: 1000\nFake videos: 1000\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport os\n\n# parameters (small on purpose)\nNUM_FRAMES = 20\nIMG_SIZE = 224\n\nDATASET_PATH = \"/kaggle/input/ff-c23/FaceForensics++_C23\"\nREAL_DIR = os.path.join(DATASET_PATH, \"original\")\n\n# pick ONE real video\nvideo_path = os.path.join(REAL_DIR, os.listdir(REAL_DIR)[0])\nprint(\"Using video:\", os.path.basename(video_path))\n\ndef extract_frames(video_path, num_frames=NUM_FRAMES):\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    frame_idxs = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n    frames = []\n\n    for idx in frame_idxs:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n        frame = frame / 255.0\n        frames.append(frame)\n\n    cap.release()\n\n    frames = np.array(frames, dtype=np.float32)\n    return frames\n\nframes = extract_frames(video_path)\n\nprint(\"Extracted frames shape:\", frames.shape)\nprint(\"Min pixel value:\", frames.min())\nprint(\"Max pixel value:\", frames.max())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T20:31:20.978292Z","iopub.execute_input":"2025-12-19T20:31:20.978630Z","iopub.status.idle":"2025-12-19T20:31:23.784243Z","shell.execute_reply.started":"2025-12-19T20:31:20.978604Z","shell.execute_reply":"2025-12-19T20:31:23.783435Z"}},"outputs":[{"name":"stdout","text":"Using video: 123.mp4\nExtracted frames shape: (20, 224, 224, 3)\nMin pixel value: 0.0\nMax pixel value: 1.0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Video\n  ↓\n  \nOn-the-fly frame sampling  ✅\n  ↓\n  \nCNN input-ready tensors\n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\n\nDATASET_PATH = \"/kaggle/input/ff-c23/FaceForensics++_C23\"\nREAL_DIR = os.path.join(DATASET_PATH, \"original\")\nFAKE_DIR = os.path.join(DATASET_PATH, \"Deepfakes\")\n\n# reuse the SAME parameters as before\nNUM_FRAMES = 20\nIMG_SIZE = 224\n\n# reuse the SAME extract_frames function\ndef extract_frames(video_path, num_frames=NUM_FRAMES):\n    import cv2\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_idxs = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n\n    frames = []\n    for idx in frame_idxs:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n        frames.append(frame / 255.0)\n\n    cap.release()\n    while len(frames) < num_frames:\n        frames.append(frames[-1])\n\n    return np.array(frames, dtype=np.float32)\n\n# ---- load ONE real and ONE fake ----\nX = []\ny = []\n\nreal_video = os.path.join(REAL_DIR, os.listdir(REAL_DIR)[0])\nfake_video = os.path.join(FAKE_DIR, os.listdir(FAKE_DIR)[0])\n\nX.append(extract_frames(real_video))\ny.append(0)   # REAL\n\nX.append(extract_frames(fake_video))\ny.append(1)   # FAKE\n\nX = np.array(X)\ny = np.array(y)\n\nprint(\"X shape:\", X.shape)\nprint(\"y labels:\", y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T20:32:54.747113Z","iopub.execute_input":"2025-12-19T20:32:54.748061Z","iopub.status.idle":"2025-12-19T20:33:10.563816Z","shell.execute_reply.started":"2025-12-19T20:32:54.748026Z","shell.execute_reply":"2025-12-19T20:33:10.563128Z"}},"outputs":[{"name":"stdout","text":"X shape: (2, 20, 224, 224, 3)\ny labels: [0 1]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import (\n    Input,\n    Dense,\n    Dropout,\n    TimeDistributed,\n    GlobalAveragePooling2D,\n    GlobalAveragePooling1D\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# parameters must MATCH previous steps\nNUM_FRAMES = 20\nIMG_SIZE = 224\n\n# ---- base CNN (frozen) ----\nbase_cnn = MobileNetV2(\n    include_top=False,\n    weights=\"imagenet\",\n    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n)\nbase_cnn.trainable = False  # baseline: do NOT unfreeze\n\n# ---- video input ----\nvideo_input = Input(shape=(NUM_FRAMES, IMG_SIZE, IMG_SIZE, 3))\n\n# ---- spatial feature extraction ----\nx = TimeDistributed(base_cnn)(video_input)\nx = TimeDistributed(GlobalAveragePooling2D())(x)\n\n# ---- temporal aggregation ----\nx = GlobalAveragePooling1D()(x)\n\n# ---- classifier ----\nx = Dense(128, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\noutput = Dense(1, activation=\"sigmoid\")(x)\n\n# ---- model ----\nmodel = Model(video_input, output)\n\n# ---- compile (no training yet) ----\nmodel.compile(\n    optimizer=Adam(1e-4),\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T20:33:37.258006Z","iopub.execute_input":"2025-12-19T20:33:37.258316Z","iopub.status.idle":"2025-12-19T20:33:39.981001Z","shell.execute_reply.started":"2025-12-19T20:33:37.258290Z","shell.execute_reply":"2025-12-19T20:33:39.980405Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1766176417.386915      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,   │             \u001b[38;5;34m0\u001b[0m │\n│                                 │ \u001b[38;5;34m3\u001b[0m)                     │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m) │     \u001b[38;5;34m2,257,984\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m1280\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m163,968\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,422,081\u001b[0m (9.24 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,422,081</span> (9.24 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m164,097\u001b[0m (641.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,097</span> (641.00 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n</pre>\n"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"history = model.fit(\n    X,\n    y,\n    epochs=1,\n    batch_size=1,\n    verbose=1\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T20:34:29.897069Z","iopub.execute_input":"2025-12-19T20:34:29.897410Z","iopub.status.idle":"2025-12-19T20:36:25.284898Z","shell.execute_reply.started":"2025-12-19T20:34:29.897381Z","shell.execute_reply":"2025-12-19T20:36:25.284213Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1766176539.202751     146 service.cc:152] XLA service 0x7dfddc0901f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1766176539.202819     146 service.cc:160]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1766176556.415325     146 cuda_dnn.cc:529] Loaded cuDNN version 91002\nI0000 00:00:1766176569.864745     146 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.2913\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os\nimport numpy as np\n\n# use same settings\nNUM_FRAMES = 20\nIMG_SIZE = 224\n\nDATASET_PATH = \"/kaggle/input/ff-c23/FaceForensics++_C23\"\nREAL_DIR = os.path.join(DATASET_PATH, \"original\")\nFAKE_DIR = os.path.join(DATASET_PATH, \"Deepfakes\")\n\n# reuse extract_frames from before\ndef extract_frames(video_path, num_frames=NUM_FRAMES):\n    import cv2\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_idxs = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n\n    frames = []\n    for idx in frame_idxs:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n        frames.append(frame / 255.0)\n\n    cap.release()\n    while len(frames) < num_frames:\n        frames.append(frames[-1])\n\n    return np.array(frames, dtype=np.float32)\n\n# ---- load 4 real + 4 fake ----\nX, y = [], []\n\nfor v in os.listdir(REAL_DIR)[:4]:\n    X.append(extract_frames(os.path.join(REAL_DIR, v)))\n    y.append(0)\n\nfor v in os.listdir(FAKE_DIR)[:4]:\n    X.append(extract_frames(os.path.join(FAKE_DIR, v)))\n    y.append(1)\n\nX = np.array(X)\ny = np.array(y)\n\nprint(\"X shape:\", X.shape)\nprint(\"y:\", y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T20:37:45.802692Z","iopub.execute_input":"2025-12-19T20:37:45.803278Z","iopub.status.idle":"2025-12-19T20:38:38.156524Z","shell.execute_reply.started":"2025-12-19T20:37:45.803245Z","shell.execute_reply":"2025-12-19T20:38:38.155902Z"}},"outputs":[{"name":"stdout","text":"X shape: (8, 20, 224, 224, 3)\ny: [0 0 0 0 1 1 1 1]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# recompile just to be safe (clean state)\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-4),\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n\nhistory = model.fit(\n    X,\n    y,\n    epochs=5,\n    batch_size=2,\n    verbose=1\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T20:38:43.674314Z","iopub.execute_input":"2025-12-19T20:38:43.675167Z","iopub.status.idle":"2025-12-19T20:40:40.761311Z","shell.execute_reply.started":"2025-12-19T20:38:43.675122Z","shell.execute_reply":"2025-12-19T20:40:40.760610Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 56ms/step - accuracy: 0.2667 - loss: 1.2304  \nEpoch 2/5\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7333 - loss: 0.6709\nEpoch 3/5\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7667 - loss: 0.3917\nEpoch 4/5\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2167 - loss: 0.8500   \nEpoch 5/5\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8167 - loss: 0.5201\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# X shape: (N, 20, 224, 224, 3)\n# y shape: (N,)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X,\n    y,\n    test_size=0.25,\n    random_state=42,\n    stratify=y\n)\n\nprint(\"Train shape:\", X_train.shape, y_train.shape)\nprint(\"Val shape:\", X_val.shape, y_val.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# X shape: (N, 20, 224, 224, 3)\n# y shape: (N,)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X,\n    y,\n    test_size=0.25,\n    random_state=42,\n    stratify=y\n)\n\nprint(\"Train shape:\", X_train.shape, y_train.shape)\nprint(\"Val shape:\", X_val.shape, y_val.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport subprocess\nfrom pathlib import Path\n\n# ---------------- CONFIG ----------------\nDATASET_PATH = \"/kaggle/input/ff-c23/FaceForensics++_C23\"\nOUTPUT_PATH = \"/kaggle/working/processed_frames\"\n\nFRAMES_PER_VIDEO = 50\nVIDEO_EXTENSIONS = (\".mp4\", \".avi\", \".mov\")\n\nREAL_FOLDER = \"original\"\nFAKE_FOLDERS = [\n    \"Deepfakes\",\n    \"Face2Face\",\n    \"FaceSwap\",\n    \"FaceShifter\",\n    \"NeuralTextures\"\n]\n\nos.makedirs(OUTPUT_PATH, exist_ok=True)\n\n# ------------- FFmpeg FUNCTION -------------\ndef extract_uniform_frames(video_path, out_dir, frames=FRAMES_PER_VIDEO):\n    os.makedirs(out_dir, exist_ok=True)\n\n    cmd = [\n        \"ffmpeg\",\n        \"-i\", video_path,\n        \"-vf\", f\"select='eq(pict_type\\\\,I)+not(eq(pict_type\\\\,I))',\"\n               f\"scale=224:224\",\n        \"-vsync\", \"vfr\",\n        \"-frames:v\", str(frames),\n        os.path.join(out_dir, \"frame_%04d.jpg\"),\n        \"-loglevel\", \"error\"\n    ]\n\n    subprocess.run(cmd, check=False)\n\n# ------------- REAL VIDEOS -------------\nreal_src = os.path.join(DATASET_PATH, REAL_FOLDER)\nreal_dst = os.path.join(OUTPUT_PATH, \"real\")\n\nfor video in os.listdir(real_src):\n    if video.lower().endswith(VIDEO_EXTENSIONS):\n        extract_uniform_frames(\n            os.path.join(real_src, video),\n            os.path.join(real_dst, Path(video).stem)\n        )\n\n# ------------- FAKE VIDEOS -------------\nfor folder in FAKE_FOLDERS:\n    fake_src = os.path.join(DATASET_PATH, folder)\n    fake_dst = os.path.join(OUTPUT_PATH, \"fake\")\n\n    for video in os.listdir(fake_src):\n        if video.lower().endswith(VIDEO_EXTENSIONS):\n            extract_uniform_frames(\n                os.path.join(fake_src, video),\n                os.path.join(fake_dst, Path(video).stem)\n            )\n\nprint(\"✅ 50 uniformly sampled frames per video extracted & labeled\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T20:43:17.311517Z","iopub.execute_input":"2025-12-19T20:43:17.312222Z","iopub.status.idle":"2025-12-19T21:22:22.884660Z","shell.execute_reply.started":"2025-12-19T20:43:17.312190Z","shell.execute_reply":"2025-12-19T21:22:22.883754Z"}},"outputs":[{"name":"stdout","text":"✅ 50 uniformly sampled frames per video extracted & labeled\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import shutil\nimport os\n\nSOURCE_DIR = \"/kaggle/working/processed_frames\"\nZIP_PATH = \"/kaggle/working/processed_frames_50frames.zip\"\n\n# Create zip\nshutil.make_archive(\n    ZIP_PATH.replace(\".zip\", \"\"),\n    'zip',\n    SOURCE_DIR\n)\n\nprint(\"✅ ZIP created at:\", ZIP_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T21:27:09.347345Z","iopub.execute_input":"2025-12-19T21:27:09.347787Z","iopub.status.idle":"2025-12-19T21:27:33.618296Z","shell.execute_reply.started":"2025-12-19T21:27:09.347734Z","shell.execute_reply":"2025-12-19T21:27:33.617493Z"}},"outputs":[{"name":"stdout","text":"✅ ZIP created at: /kaggle/working/processed_frames_50frames.zip\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!zip -r processed_frames.zip /kaggle/working/processed_frames_50frames.zip\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T21:37:18.177077Z","iopub.execute_input":"2025-12-19T21:37:18.177780Z","iopub.status.idle":"2025-12-19T21:37:21.479805Z","shell.execute_reply.started":"2025-12-19T21:37:18.177746Z","shell.execute_reply":"2025-12-19T21:37:21.478847Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/processed_frames_50frames.zip (stored 0%)\n","output_type":"stream"}],"execution_count":17}]}